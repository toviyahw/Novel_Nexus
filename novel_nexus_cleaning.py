# -*- coding: utf-8 -*-
"""Novel Nexus Cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18e5STfBaP0moFzo4YB5DbwAJcsd6JO2I

# **Cleaning Initial Excel Datasets**

After manually inputting my data into Excel sheets, I used the pandas library to check for duplicates and any NULL values. Because all of my files are separated into what will be normalized SQL tables, there can be neither.

I am also changing the configuration of IPython interactive shell from the default 'last_expr' to 'all' so that I can see the outputs of all of my expressions instead of just the final one.
"""

import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all" #prints all commands

#authors table

df = pd.read_csv('authors.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#books table

df = pd.read_csv('books.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

"""Here, the **page_count** column has 14 NULL values. These NULL values represent books I have read that have no pages, in other words, the audiobooks. To resolve this, I will replace all NULL values here with 0."""

df.fillna(0, inplace= True)

"""Now all of my page count values that were previously NULL have been assigned a value of 0."""

#date_read table

df = pd.read_csv('date_read.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#format table

df = pd.read_csv('format.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#genres table

df = pd.read_csv('genres.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#gr_scores table

df = pd.read_csv('gr_scores.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#my_library table

df = pd.read_csv('my_library.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#ratings table

df = pd.read_csv('ratings.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

#series table

df = pd.read_csv('series.csv')

df.head()
df.isnull().sum()

num_of_duplicates = df.duplicated().sum()
print (f'This dataset contains {num_of_duplicates} duplicates.')

"""Now that I have handled the NULL values and confirmed that my dataset is free of duplicates, I can begin creating my tables using postgreSQL with clean datasets."""